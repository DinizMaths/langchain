{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a digital ledger that stores information in blocks that are linked together in a chain. Each block contains a list of transactions, and once a block is added to the chain, it cannot be altered or deleted. This makes blockchain secure and transparent, as all transactions are recorded and verified by a network of computers. It is commonly used for cryptocurrencies like Bitcoin, but can also be used for other types of transactions and data storage.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "output = llm.invoke(\"Explain the concept of blockchain in simple terms\")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blockchain is a digital ledger that stores information in blocks that are linked together in a chain. Each block contains a list of transactions, and once a block is added to the chain, it cannot be altered or deleted. This makes blockchain secure and transparent, as all transactions are recorded and verified by a network of computers. It is commonly used for cryptocurrencies like Bitcoin, but can also be used for other types of transactions and data storage.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "output = llm.invoke(\"Explain the concept of blockchain in simple terms\")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um blockchain é um registro digital de transações que é armazenado em vários computadores em uma rede descentralizada.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a blockchain expert that respons every question with a simple answer in portuguese.\"),\n",
    "    HumanMessage(content=\"What is blockchain?\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why couldn't the pirate play cards?\n",
      "\n",
      "Because he was sitting on the deck!\n",
      "CPU times: user 7.1 ms, sys: 0 ns, total: 7.1 ms\n",
      "Wall time: 901 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "prompt = \"Tell me a pirate joke\"\n",
    "\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why couldn't the pirate play cards?\n",
      "\n",
      "Because he was sitting on the deck!\n",
      "CPU times: user 456 μs, sys: 0 ns, total: 456 μs\n",
      "Wall time: 451 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"Tell me a pirate joke\"\n",
    "\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why did the pirate go to therapy?\n",
      "\n",
      "Because he was feeling arrrrrrrr-nxious!\n",
      "CPU times: user 36.9 ms, sys: 0 ns, total: 36.9 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache())\n",
    "\n",
    "prompt = \"Tell me a pirate joke\"\n",
    "\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why did the pirate go to therapy?\n",
      "\n",
      "Because he was feeling arrrrrrrr-nxious!\n",
      "CPU times: user 5.33 ms, sys: 0 ns, total: 5.33 ms\n",
      "Wall time: 4.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"Tell me a pirate joke\"\n",
    "\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "In a field of flowers, I found a rock\n",
      "Among the petals, it stood out in shock\n",
      "A symbol of strength, solid and true\n",
      "Just like my love, forever for you\n",
      "\n",
      "Chorus:\n",
      "Flowers and rocks, a perfect pair\n",
      "Beauty and strength, beyond compare\n",
      "In your arms, I feel complete\n",
      "Our love, like flowers and rocks, so sweet\n",
      "\n",
      "Verse 2:\n",
      "The flowers bloom, the rocks remain\n",
      "Through storms and sunshine, they both sustain\n",
      "Just like our love, through highs and lows\n",
      "Together we'll weather all the blows\n",
      "\n",
      "Chorus:\n",
      "Flowers and rocks, a perfect pair\n",
      "Beauty and strength, beyond compare\n",
      "In your arms, I feel complete\n",
      "Our love, like flowers and rocks, so sweet\n",
      "\n",
      "Bridge:\n",
      "With petals soft and colors bright\n",
      "And rocks so solid, a steadfast might\n",
      "Together they create a perfect blend\n",
      "Just like us, my love, until the end\n",
      "\n",
      "Chorus:\n",
      "Flowers and rocks, a perfect pair\n",
      "Beauty and strength, beyond compare\n",
      "In your arms, I feel complete\n",
      "Our love, like flowers and rocks, so sweet\n",
      "\n",
      "Outro:\n",
      "So let's bloom like flowers and stand like rocks\n",
      "Facing the world with love that never stops\n",
      "Together we'll weather all life brings\n",
      "Our love, like flowers and rocks, forever sings.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a love song about flowers and rocks.\"\n",
    "\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "In a field of flowers, I found you\n",
      "Among the rocks, you stood so true\n",
      "Your beauty blooms like a rose in June\n",
      "A love so strong, it will never swoon\n",
      "\n",
      "Chorus:\n",
      "You're my flower in a world of rocks\n",
      "You're my anchor in the stormy docks\n",
      "Together we'll weather any storm\n",
      "Our love will always keep us warm\n",
      "\n",
      "Verse 2:\n",
      "Among the rocks, our love will grow\n",
      "Like a wildflower, it will show\n",
      "Through the toughest times, we will stand\n",
      "Hand in hand, we'll conquer all we can\n",
      "\n",
      "Chorus:\n",
      "You're my flower in a world of rocks\n",
      "You're my anchor in the stormy docks\n",
      "Together we'll weather any storm\n",
      "Our love will always keep us warm\n",
      "\n",
      "Bridge:\n",
      "Like a rock that never crumbles\n",
      "Our love will never stumble\n",
      "Through the trials and the pain\n",
      "Our love will always remain\n",
      "\n",
      "Chorus:\n",
      "You're my flower in a world of rocks\n",
      "You're my anchor in the stormy docks\n",
      "Together we'll weather any storm\n",
      "Our love will always keep us warm\n",
      "\n",
      "Outro:\n",
      "In a field of flowers, I found you\n",
      "Among the rocks, our love is true\n",
      "Forever blooming, forever strong\n",
      "Our love will last a lifetime long."
     ]
    }
   ],
   "source": [
    "prompt = \"Write a love song about flowers and rocks.\"\n",
    "\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''You are a code expert that responds every question with a simple answer in portuguese.\n",
    "Write a few lines of code that prints \"Hello, World!\" in {language} using comments in {comment_language}.'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a code expert that responds every question with a simple answer in portuguese.\\nWrite a few lines of code that prints \"Hello, World!\" in Python using comments in Japanese.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(language=\"Python\", comment_language=\"Japanese\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a code expert that responds every question with a simple answer in portuguese.\\nWrite a few lines of code that prints \"Hello, World!\" in Python using comments in Japanese.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# こんにちは、世界！\n",
      "print(\"Hello, World!\")\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You respond only in json format.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Top {n} states in {country} by {metric}.\")\n",
    "])\n",
    "\n",
    "chat = chat_template.format(n=5, country=\"Brazil\", metric=\"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You respond only in json format.\\nHuman: Top 5 states in Brazil by population.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": \"Sao Paulo\",\n",
      "    \"2\": \"Minas Gerais\",\n",
      "    \"3\": \"Rio de Janeiro\",\n",
      "    \"4\": \"Bahia\",\n",
      "    \"5\": \"Parana\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "output = llm.invoke(chat)\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "template = '''You are a code expert that responds every question with a simple answer in portuguese.\n",
    "Write a few lines of code that prints \"Hello, World!\" in {language} using comments in {comment_language}.'''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['comment_language', 'language'], template='You are a code expert that responds every question with a simple answer in portuguese.\\nWrite a few lines of code that prints \"Hello, World!\" in {language} using comments in {comment_language}.'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f7974ce3c40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f7974cfd330>, openai_api_key=SecretStr('**********'), openai_proxy=''))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a code expert that responds every question with a simple answer in portuguese.\n",
      "Write a few lines of code that prints \"Hello, World!\" in Java using comments in Portuguese.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'language': 'Java', 'comment_language': 'Portuguese', 'text': '// Declaração da classe\\npublic class HelloWorld {\\n\\n    // Método principal\\n    public static void main(String[] args) {\\n        // Imprimir \"Hello, World!\"\\n        System.out.println(\"Hello, World!\");\\n    }\\n}'}\n"
     ]
    }
   ],
   "source": [
    "language = input(\"Enter the programming language: \")\n",
    "comment_language = input(\"Enter the comment language: \")\n",
    "\n",
    "output = chain.invoke({\"language\": language, \"comment_language\": comment_language})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Declaração da classe\n",
      "public class HelloWorld {\n",
      "\n",
      "    // Método principal\n",
      "    public static void main(String[] args) {\n",
      "        // Imprimir \"Hello, World!\"\n",
      "        System.out.println(\"Hello, World!\");\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_1 = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "prompt_template_1 = PromptTemplate.from_template(\n",
    "    template=\"You are an expert in Python programming that responds every question with a simple answer in portuguese. Write a function that implements the concept of {concept}.\"\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(\n",
    "    llm=llm_1,\n",
    "    prompt=prompt_template_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_2 = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=1.2\n",
    ")\n",
    "\n",
    "prompt_template_2 = PromptTemplate.from_template(\n",
    "    template=\"Given the following code in Python:\\d {code} \\ddocument the code in portuguese.\"\n",
    ")\n",
    "\n",
    "chain_2 = LLMChain(\n",
    "    llm=llm_2,\n",
    "    prompt=prompt_template_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mdef recursao(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * recursao(n-1)\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mClaro, aqui está a documentação do código em português:\n",
      "\n",
      "```python\n",
      "def recursao(n):\n",
      "    \"\"\"\n",
      "    Função recursiva que calcula o fatorial de um número inteiro n.\n",
      "    \n",
      "    Parâmetros:\n",
      "    n (int): O número para o qual o fatorial será calculado. \n",
      "             Deve ser um inteiro não negativo.\n",
      "    \n",
      "    Retorna:\n",
      "    int: O valor do fatorial de n.\n",
      "    \"\"\"\n",
      "    if n == 0:\n",
      "        # Caso base: o fatorial de 0 é 1\n",
      "        return 1\n",
      "    else:\n",
      "        # Caso recursivo: o fatorial de n é n vezes o fatorial de (n-1)\n",
      "        return n * recursao(n-1)\n",
      "```\n",
      "\n",
      "### Descrição do código:\n",
      "- A função `recursao` calcula o fatorial de um número `n` de maneira recursiva.\n",
      "- O caso base da recursão é quando `n` é igual a 0, retornando 1 (por definição, 0! = 1).\n",
      "- Para outros valores de `n`, a função retorna `n` multiplicado pelo resultado da chamada da própria função com o argumento `n-1`.\n",
      "- Isso continua até que o caso base seja atingido.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'recursion', 'output': 'Claro, aqui está a documentação do código em português:\\n\\n```python\\ndef recursao(n):\\n    \"\"\"\\n    Função recursiva que calcula o fatorial de um número inteiro n.\\n    \\n    Parâmetros:\\n    n (int): O número para o qual o fatorial será calculado. \\n             Deve ser um inteiro não negativo.\\n    \\n    Retorna:\\n    int: O valor do fatorial de n.\\n    \"\"\"\\n    if n == 0:\\n        # Caso base: o fatorial de 0 é 1\\n        return 1\\n    else:\\n        # Caso recursivo: o fatorial de n é n vezes o fatorial de (n-1)\\n        return n * recursao(n-1)\\n```\\n\\n### Descrição do código:\\n- A função `recursao` calcula o fatorial de um número `n` de maneira recursiva.\\n- O caso base da recursão é quando `n` é igual a 0, retornando 1 (por definição, 0! = 1).\\n- Para outros valores de `n`, a função retorna `n` multiplicado pelo resultado da chamada da própria função com o argumento `n-1`.\\n- Isso continua até que o caso base seja atingido.'}\n"
     ]
    }
   ],
   "source": [
    "output = overall_chain.invoke(\"recursion\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aqui está a documentação do código em português:\n",
      "\n",
      "```python\n",
      "def recursao(n):\n",
      "    \"\"\"\n",
      "    Função recursiva que calcula o fatorial de um número inteiro n.\n",
      "    \n",
      "    Parâmetros:\n",
      "    n (int): O número para o qual o fatorial será calculado. \n",
      "             Deve ser um inteiro não negativo.\n",
      "    \n",
      "    Retorna:\n",
      "    int: O valor do fatorial de n.\n",
      "    \"\"\"\n",
      "    if n == 0:\n",
      "        # Caso base: o fatorial de 0 é 1\n",
      "        return 1\n",
      "    else:\n",
      "        # Caso recursivo: o fatorial de n é n vezes o fatorial de (n-1)\n",
      "        return n * recursao(n-1)\n",
      "```\n",
      "\n",
      "### Descrição do código:\n",
      "- A função `recursao` calcula o fatorial de um número `n` de maneira recursiva.\n",
      "- O caso base da recursão é quando `n` é igual a 0, retornando 1 (por definição, 0! = 1).\n",
      "- Para outros valores de `n`, a função retorna `n` multiplicado pelo resultado da chamada da própria função com o argumento `n-1`.\n",
      "- Isso continua até que o caso base seja atingido.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-udemy-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
